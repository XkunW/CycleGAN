{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "UNIT_hyperparameter_training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_fL9DbEOEz0T"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bubiSTEATVm"
      },
      "source": [
        "### Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ3reL0j1vaI"
      },
      "source": [
        "%%capture\n",
        "!pip install torchfile \n",
        "!pip install tensorboardX\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiXc1vOwAWon"
      },
      "source": [
        "### Git Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRXgtAUz1Rk2",
        "outputId": "a50e2026-8da1-4555-a7de-c7b4920caa71"
      },
      "source": [
        "# Clone git repository \n",
        "!git clone 'https://github.com/XkunW/Image_Translation.git'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Image_Translation'...\n",
            "remote: Enumerating objects: 381, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 381 (delta 50), reused 53 (delta 23), pack-reused 292\n",
            "Receiving objects: 100% (381/381), 382.56 MiB | 25.37 MiB/s, done.\n",
            "Resolving deltas: 100% (159/159), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vhBSMUX_kGo",
        "outputId": "a5b84f28-ccf7-42bf-f523-3c7e5fb6881c"
      },
      "source": [
        "! git pull\n",
        "# ! git status\n",
        "# ! git checkout utils.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW4_po62Aaoa"
      },
      "source": [
        "### Drive mounting and unzipping data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmvJIoBD7NWj",
        "outputId": "273aa321-cced-47f6-dae1-c9c5f519ef59"
      },
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "data_dir = '/content/drive/MyDrive/CSC2516_Project/Datasets/' #same for Tina and Sophie\n",
        "\n",
        "# data zip \n",
        "summer2winter = data_dir+'summer2winter_yosemite_small_dataset.zip' \n",
        "monet2photo = data_dir+'monet2photo_small_dataset.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9ACMSzUY51-M",
        "outputId": "47267013-182e-4ad4-cdba-a70cc5c5e7bd"
      },
      "source": [
        "# change to UNIT folder\n",
        "#%cd '/content/Image_Translation/UNIT'\n",
        "os.chdir('Image_Translation/UNIT')\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/Image_Translation/UNIT'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWhcL43LGtlP"
      },
      "source": [
        "# Unzipping datasets to the target folder\n",
        "%%capture\n",
        "!unzip \"$summer2winter\" -d '/content/Image_Translation/UNIT/datasets/'\n",
        "#!unzip \"$monet2photo\" -d '/content/Image_Translation/UNIT/datasets/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRtc6tY5HA4R"
      },
      "source": [
        "# copy vgg16 model weights into the models folder in github repo\n",
        "!cp \"/content/drive/MyDrive/CSC2516_Project/UNIT_colab/VGG_model/vgg16.weight\" \"/content/Image_Translation/UNIT/models\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AldsflLhAzhv"
      },
      "source": [
        "### Training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUssvD0mMMnt"
      },
      "source": [
        "from utils import get_all_data_loaders, prepare_sub_folder, write_html, write_loss, get_config, write_2images\n",
        "import argparse\n",
        "from trainer import UNIT_Trainer\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch\n",
        "try:\n",
        "    from itertools import izip as zip\n",
        "except ImportError: # will be 3.x series\n",
        "    pass\n",
        "import sys\n",
        "from tensorboardX import SummaryWriter\n",
        "import shutil\n",
        "from tqdm import tqdm "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I58vBjUgMORq"
      },
      "source": [
        "# Choose the configuration file to use for \n",
        "\n",
        "# configs/unit_summer2winter_yosemite256_folder.yaml\n",
        "# configs/unit_summer2winter_yosemite256_folder.yaml\n",
        "config_file = 'configs/unit_summer2winter_yosemite256_folder.yaml'\n",
        "output_path = '/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small'\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--config', type=str, default=config_file, help='Path to the config file.')\n",
        "parser.add_argument('--output_path', type=str, default=output_path, help=\"outputs path\")\n",
        "# parser.add_argument(\"--resume\", action=\"store_true\")\n",
        "parser.add_argument('--trainer', type=str, default='UNIT', help=\"UNIT\")\n",
        "parser.add_argument('-f')\n",
        "opts = parser.parse_args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fL9DbEOEz0T"
      },
      "source": [
        "### loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrk2zOnp2ewa",
        "outputId": "603ec90c-5502-44fa-a6c9-66b357ae458c"
      },
      "source": [
        "cudnn.benchmark = True\n",
        "resume = False\n",
        "\n",
        "# Load experiment setting\n",
        "config = get_config(opts.config)\n",
        "display_size = config['display_size']\n",
        "config['vgg_model_path'] = '.'\n",
        "max_iter = 80000\n",
        "config['max_iter'] = max_iter\n",
        "config['snapshot_save_iter'] = int(max_iter / 10)  \n",
        "config['image_save_iter'] =  int(max_iter / 10)\n",
        "config['image_display_iter'] =  int(max_iter / 10)  \n",
        "\n",
        "skip_val = [(0.01,0.01), (0.01,0.1), (0.1,0.01)]\n",
        "batch_sizes = [1] #[1, 4]\n",
        "learning_rate = [0.0001]#, 0.0005] #initial learning rate\n",
        "recon_kl_weight = [0.01, 0.1] #since: in paper the authors found setting the weights of the KL terms to 0.1 resulted in consistently good performance\n",
        "recon_kl_cyc_weight = [0.01, 0.1]\n",
        "\n",
        "for batch_size_val in batch_sizes:\n",
        "  for lr_val in learning_rate:\n",
        "      for recon_kl_w in recon_kl_weight:\n",
        "          for recon_kl_cyc_w in recon_kl_cyc_weight:\n",
        "              \n",
        "              continue_Flag = False\n",
        "              for kl_weight, kl_cyc_weight in skip_val:\n",
        "                if recon_kl_w == kl_weight and recon_kl_cyc_w == kl_cyc_weight:\n",
        "                  continue_Flag = True\n",
        "                  print('continue of %.2f, %.2f' % (kl_weight, kl_cyc_weight))\n",
        "                  break\n",
        "              \n",
        "              if continue_Flag:\n",
        "                continue\n",
        "                \n",
        "              config['batch_size'] = batch_size_val\n",
        "              config['lr'] = lr_val\n",
        "              config['recon_kl_w'] = recon_kl_w\n",
        "              config['recon_kl_cyc_w'] = recon_kl_cyc_w\n",
        "              trainer = UNIT_Trainer(config)\n",
        "              param_values = 'batch_size_'+ str(batch_size_val) \\\n",
        "              + '_recon_kl_w_' + str(recon_kl_w) \\\n",
        "              + '_recon_kl_clc_'+ str(recon_kl_cyc_w) \\\n",
        "              + '_lr_value_' + str(lr_val) \\\n",
        "\n",
        "              if torch.cuda.is_available():\n",
        "                  trainer.cuda()\n",
        "\n",
        "              train_loader_a, train_loader_b, test_loader_a, test_loader_b = get_all_data_loaders(config)\n",
        "              if torch.cuda.is_available():\n",
        "                  train_display_images_a = torch.stack([train_loader_a.dataset[i] for i in range(display_size)]).cuda()\n",
        "                  train_display_images_b = torch.stack([train_loader_b.dataset[i] for i in range(display_size)]).cuda()\n",
        "                  test_display_images_a = torch.stack([test_loader_a.dataset[i] for i in range(display_size)]).cuda()\n",
        "                  test_display_images_b = torch.stack([test_loader_b.dataset[i] for i in range(display_size)]).cuda()\n",
        "              else:\n",
        "                  train_display_images_a = torch.stack([train_loader_a.dataset[i] for i in range(display_size)])\n",
        "                  train_display_images_b = torch.stack([train_loader_b.dataset[i] for i in range(display_size)])\n",
        "                  test_display_images_a = torch.stack([test_loader_a.dataset[i] for i in range(display_size)])\n",
        "                  test_display_images_b = torch.stack([test_loader_b.dataset[i] for i in range(display_size)])\n",
        "\n",
        "              # Setup logger and output folders, no need during hp tuning \n",
        "              model_name = os.path.splitext(os.path.basename(opts.config))[0]\n",
        "              train_writer = SummaryWriter(os.path.join(opts.output_path + \"/logs\", model_name)) # + '_' + param_values))\n",
        "              output_directory = os.path.join(opts.output_path + \"/outputs\", model_name) #+ '_' + param_values)\n",
        "              checkpoint_directory, image_directory = prepare_sub_folder(output_directory)\n",
        "              shutil.copy(opts.config, os.path.join(output_directory, 'config.yaml')) # copy config file to output folder\n",
        "\n",
        "              # Start training\n",
        "              iterations = trainer.resume(checkpoint_directory, hyperparameters=config) if resume else 0\n",
        "\n",
        "              training_complete = False\n",
        "              while not training_complete:\n",
        "                  for it, (images_a, images_b) in enumerate(tqdm(zip(train_loader_a, train_loader_b))):\n",
        "                      trainer.update_learning_rate()\n",
        "                      if torch.cuda.is_available():\n",
        "                          images_a, images_b = images_a.cuda().detach(), images_b.cuda().detach()\n",
        "                      else:\n",
        "                          images_a, images_b = images_a.detach(), images_b.detach()\n",
        "\n",
        "                      # Main training code\n",
        "                      trainer.dis_update(images_a, images_b, config)\n",
        "                      trainer.gen_update(images_a, images_b, config)\n",
        "                      if torch.cuda.is_available():\n",
        "                          torch.cuda.synchronize()\n",
        "\n",
        "                      # Dump training stats in log file\n",
        "                      if (iterations + 1) % config['log_iter'] == 0:\n",
        "                          write_loss(iterations, trainer, train_writer)\n",
        "\n",
        "                      # Write images\n",
        "                      if (iterations + 1) % config['image_save_iter'] == 0:\n",
        "                          with torch.no_grad():\n",
        "                              test_image_outputs = trainer.sample(test_display_images_a, test_display_images_b)\n",
        "                              train_image_outputs = trainer.sample(train_display_images_a, train_display_images_b)\n",
        "                          \n",
        "                          #the last input is the postfix for filename - edit this so we can store images from different configs\n",
        "                          write_2images(test_image_outputs, display_size, image_directory, 'test_%08d_%s' % (iterations + 1, param_values))\n",
        "                          write_2images(train_image_outputs, display_size, image_directory, 'train_%08d_%s' % (iterations + 1, param_values))\n",
        "                          # HTML\n",
        "                          write_html(output_directory + \"/index.html\", iterations + 1, config['image_save_iter'], 'images', param_values)\n",
        "\n",
        "                      if (iterations + 1) % config['image_display_iter'] == 0:\n",
        "                          with torch.no_grad():\n",
        "                              image_outputs = trainer.sample(train_display_images_a, train_display_images_b)\n",
        "                          write_2images(image_outputs, display_size, image_directory, 'train_current_' + param_values)\n",
        "\n",
        "                      # Save network weights\n",
        "                      if (iterations + 1) % config['snapshot_save_iter'] == 0:    # save iter 0 sanity check\n",
        "                          trainer.save(checkpoint_directory, iterations, param_values)\n",
        "\n",
        "                      iterations += 1\n",
        "                      if iterations >= config['max_iter']:\n",
        "                          training_complete = True\n",
        "                          break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "164it [01:21,  2.01it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykAG-_WI6_-I"
      },
      "source": [
        "### Resume"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvUZAuF4bMis",
        "outputId": "eb111abe-37e2-47bb-b2c0-6fec3bf2b11f"
      },
      "source": [
        "cudnn.benchmark = True\n",
        "\n",
        "# Load experiment setting\n",
        "config = get_config(opts.config)\n",
        "display_size = config['display_size']\n",
        "config['vgg_model_path'] = '.'\n",
        "max_iter = 80000\n",
        "config['max_iter'] = max_iter\n",
        "config['snapshot_save_iter'] = int(max_iter / 10)  \n",
        "config['image_save_iter'] =  int(max_iter / 10)\n",
        "config['image_display_iter'] =  int(max_iter / 10)  \n",
        "\n",
        "#skip_val = [(0.01,0.01), (0.01,0.1), (0.1,0.01)]\n",
        "recon_kl_weight = [0.01, 0.1] #since: in paper the authors found setting the weights of the KL terms to 0.1 resulted in consistently good performance\n",
        "recon_kl_cyc_weight = [0.01, 0.1]\n",
        "\n",
        "\n",
        "batch_size_val = 1\n",
        "lr_val = 0.0001\n",
        "recon_kl_w = 0.1\n",
        "recon_kl_cyc_w = 0.01\n",
        "resume = True\n",
        "iterations = 72000\n",
        "gen_dir = '/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/gen_00072000_batch_size_1_recon_kl_w_0.1_recon_kl_clc_0.01_lr_value_0.0001.pt'\n",
        "dis_dir = '/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/dis_00072000_batch_size_1_recon_kl_w_0.1_recon_kl_clc_0.01_lr_value_0.0001.pt'\n",
        "opt_dir = '/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/optimizer_batch_size_1_recon_kl_w_0.1_recon_kl_clc_0.01_lr_value_0.0001.pt'\n",
        "  \n",
        "config['batch_size'] = batch_size_val\n",
        "config['lr'] = lr_val\n",
        "config['recon_kl_w'] = recon_kl_w\n",
        "config['recon_kl_cyc_w'] = recon_kl_cyc_w\n",
        "trainer = UNIT_Trainer(config)\n",
        "param_values = 'batch_size_'+ str(batch_size_val) \\\n",
        "+ '_recon_kl_w_' + str(recon_kl_w) \\\n",
        "+ '_recon_kl_clc_'+ str(recon_kl_cyc_w) \\\n",
        "+ '_lr_value_' + str(lr_val) \\\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    trainer.cuda()\n",
        "\n",
        "train_loader_a, train_loader_b, test_loader_a, test_loader_b = get_all_data_loaders(config)\n",
        "if torch.cuda.is_available():\n",
        "    train_display_images_a = torch.stack([train_loader_a.dataset[i] for i in range(display_size)]).cuda()\n",
        "    train_display_images_b = torch.stack([train_loader_b.dataset[i] for i in range(display_size)]).cuda()\n",
        "    test_display_images_a = torch.stack([test_loader_a.dataset[i] for i in range(display_size)]).cuda()\n",
        "    test_display_images_b = torch.stack([test_loader_b.dataset[i] for i in range(display_size)]).cuda()\n",
        "else:\n",
        "    train_display_images_a = torch.stack([train_loader_a.dataset[i] for i in range(display_size)])\n",
        "    train_display_images_b = torch.stack([train_loader_b.dataset[i] for i in range(display_size)])\n",
        "    test_display_images_a = torch.stack([test_loader_a.dataset[i] for i in range(display_size)])\n",
        "    test_display_images_b = torch.stack([test_loader_b.dataset[i] for i in range(display_size)])\n",
        "\n",
        "# Setup logger and output folders, no need during hp tuning \n",
        "model_name = os.path.splitext(os.path.basename(opts.config))[0]\n",
        "train_writer = SummaryWriter(os.path.join(opts.output_path + \"/logs\", model_name)) # + '_' + param_values))\n",
        "output_directory = os.path.join(opts.output_path + \"/outputs\", model_name) #+ '_' + param_values)\n",
        "checkpoint_directory, image_directory = prepare_sub_folder(output_directory)\n",
        "shutil.copy(opts.config, os.path.join(output_directory, 'config.yaml')) # copy config file to output folder\n",
        "\n",
        "# Start training\n",
        "# code change in resume function in trainer: iterations = int(last_model_name.split('/')[-1][4:11])\n",
        "trainer.resume(gen_dir=gen_dir, dis_dir=dis_dir, opt_dir=opt_dir, iterations=iterations, hyperparameters=config)\n",
        "\n",
        "training_complete = False\n",
        "while not training_complete:\n",
        "    for it, (images_a, images_b) in enumerate(tqdm(zip(train_loader_a, train_loader_b))):\n",
        "        trainer.update_learning_rate()\n",
        "        if torch.cuda.is_available():\n",
        "            images_a, images_b = images_a.cuda().detach(), images_b.cuda().detach()\n",
        "        else:\n",
        "            images_a, images_b = images_a.detach(), images_b.detach()\n",
        "\n",
        "        # Main training code\n",
        "        trainer.dis_update(images_a, images_b, config)\n",
        "        trainer.gen_update(images_a, images_b, config)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        # Dump training stats in log file\n",
        "        if (iterations + 1) % config['log_iter'] == 0:\n",
        "            write_loss(iterations, trainer, train_writer)\n",
        "\n",
        "        # Write images\n",
        "        if (iterations + 1) % config['image_save_iter'] == 0:\n",
        "            with torch.no_grad():\n",
        "                test_image_outputs = trainer.sample(test_display_images_a, test_display_images_b)\n",
        "                train_image_outputs = trainer.sample(train_display_images_a, train_display_images_b)\n",
        "            \n",
        "            #the last input is the postfix for filename - edit this so we can store images from different configs\n",
        "            write_2images(test_image_outputs, display_size, image_directory, 'test_%08d_%s' % (iterations + 1, param_values))\n",
        "            write_2images(train_image_outputs, display_size, image_directory, 'train_%08d_%s' % (iterations + 1, param_values))\n",
        "            # HTML\n",
        "            write_html(output_directory + \"/index.html\", iterations + 1, config['image_save_iter'], 'images', param_values)\n",
        "\n",
        "        if (iterations + 1) % config['image_display_iter'] == 0:\n",
        "            with torch.no_grad():\n",
        "                image_outputs = trainer.sample(train_display_images_a, train_display_images_b)\n",
        "            write_2images(image_outputs, display_size, image_directory, 'train_current_' + param_values)\n",
        "\n",
        "        # Save network weights\n",
        "        if (iterations + 1) % config['snapshot_save_iter'] == 0:    # save iter 0 sanity check\n",
        "            trainer.save(checkpoint_directory, iterations, param_values)\n",
        "\n",
        "        iterations += 1\n",
        "        if iterations >= config['max_iter']:\n",
        "            training_complete = True\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "hi\n",
            "Resume from iteration 72000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "400it [03:20,  1.99it/s]\n",
            "400it [03:20,  2.00it/s]\n",
            "400it [03:20,  2.00it/s]\n",
            "400it [03:20,  2.00it/s]\n",
            "400it [03:20,  2.00it/s]\n",
            "400it [03:20,  2.00it/s]\n",
            "400it [03:20,  2.00it/s]\n",
            "400it [03:20,  2.00it/s]\n",
            "400it [03:20,  2.00it/s]\n",
            "400it [03:19,  2.00it/s]\n",
            "400it [03:19,  2.00it/s]\n",
            "400it [03:19,  2.01it/s]\n",
            "400it [03:19,  2.01it/s]\n",
            "400it [03:20,  2.00it/s]\n",
            "400it [03:19,  2.00it/s]\n",
            "400it [03:19,  2.00it/s]\n",
            "400it [03:19,  2.00it/s]\n",
            "400it [03:19,  2.00it/s]\n",
            "400it [03:19,  2.00it/s]\n",
            "399it [03:18,  2.03it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hreEX-BMcdMV"
      },
      "source": [
        "trainer.resume"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTlTrIpVB4Z8"
      },
      "source": [
        "checkpoint_directory"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}