{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNIT_test_summer2winter",
      "provenance": [],
      "collapsed_sections": [
        "xNhdCuKltCS6",
        "WtEMfq5Y6lDU"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpc-vvFR0XgA"
      },
      "source": [
        "### Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btDJvhWz0QOw"
      },
      "source": [
        "%%capture\n",
        "!pip install torchfile \n",
        "!pip install tensorboardX\n",
        "!pip install pytorch-fid \n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QUd07S30a2T"
      },
      "source": [
        "## Git operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lH8s21S0cEm",
        "outputId": "758d58c3-4b7a-4bb3-aa6d-6a2252bd3225"
      },
      "source": [
        "# Clone git repository \n",
        "!git clone 'https://github.com/XkunW/Image_Translation.git'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Image_Translation'...\n",
            "remote: Enumerating objects: 381, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 381 (delta 50), reused 53 (delta 23), pack-reused 292\u001b[K\n",
            "Receiving objects: 100% (381/381), 382.56 MiB | 8.84 MiB/s, done.\n",
            "Resolving deltas: 100% (157/157), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKx40v7f0ee-",
        "outputId": "5b19a356-378b-4df6-adf7-e8506cc2c785"
      },
      "source": [
        "! git pull\n",
        "# ! git status\n",
        "# ! git checkout utils.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MTnFEZnn_jI"
      },
      "source": [
        "Clone for calculating FID score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XloUZkh_n52-"
      },
      "source": [
        "# !git clone 'https://github.com/mseitzer/pytorch-fid.git'\n",
        "# ! git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cite4Lb40igJ"
      },
      "source": [
        "## Drive mounting and unzipping data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcbCXfQK0gXx",
        "outputId": "85149755-90d3-4960-c119-57998075fd29"
      },
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "data_dir = '/content/drive/MyDrive/CSC2516_Project/Datasets/' #same for Tina and Sophie\n",
        "\n",
        "# data zip \n",
        "summer2winter = data_dir+'summer2winter_yosemite_small_dataset.zip' \n",
        "monet2photo = data_dir+'monet2photo_small_dataset.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KDa-_NxI01Uc",
        "outputId": "07dfae64-2e88-441c-d4e2-5942cc5a7b14"
      },
      "source": [
        "# change to UNIT folder\n",
        "#%cd '/content/Image_Translation/UNIT'\n",
        "os.chdir('Image_Translation/UNIT')\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/Image_Translation/UNIT'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGiX7Mn98mcm"
      },
      "source": [
        "# Unzipping datasets to the target folder\n",
        "%%capture\n",
        "!unzip \"$summer2winter\" -d '/content/Image_Translation/UNIT/datasets/'\n",
        "#!unzip \"$monet2photo\" -d '/content/Image_Translation/UNIT/datasets/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05KtCaXG8m2Z"
      },
      "source": [
        "# copy vgg16 model weights into the models folder in github repo\n",
        "!cp \"/content/drive/MyDrive/CSC2516_Project/UNIT_colab/VGG_model/vgg16.weight\" \"/content/Image_Translation/UNIT/models\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNhdCuKltCS6"
      },
      "source": [
        "## Functions for FID score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGSzzPJXtE-S"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\n",
        "from multiprocessing import cpu_count\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as TF\n",
        "from PIL import Image\n",
        "from scipy import linalg\n",
        "from torch.nn.functional import adaptive_avg_pool2d\n",
        "# try:\n",
        "#     from tqdm import tqdm\n",
        "# except ImportError:\n",
        "#     # If tqdm is not available, provide a mock version of it\n",
        "#     def tqdm(x):\n",
        "#         return x\n",
        "\n",
        "from pytorch_fid.inception import InceptionV3\n",
        "\n",
        "IMAGE_EXTENSIONS = {'bmp', 'jpg', 'jpeg', 'pgm', 'png', 'ppm',\n",
        "                    'tif', 'tiff', 'webp'}\n",
        "\n",
        "class ImagePathDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, files, transforms=None):\n",
        "        self.files = files\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        path = self.files[i]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "        return img\n",
        "\n",
        "def get_activations(files, model, batch_size=50, dims=2048, device='cpu'):\n",
        "    model.eval()\n",
        "\n",
        "    if batch_size > len(files):\n",
        "        print(('Warning: batch size is bigger than the data size. '\n",
        "               'Setting batch size to data size'))\n",
        "        batch_size = len(files)\n",
        "\n",
        "    dataset = ImagePathDataset(files, transforms=TF.ToTensor())\n",
        "    dataloader = torch.utils.data.DataLoader(dataset,\n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=False,\n",
        "                                             drop_last=False,\n",
        "                                             num_workers=cpu_count())\n",
        "\n",
        "    pred_arr = np.empty((len(files), dims))\n",
        "\n",
        "    start_idx = 0\n",
        "\n",
        "    for batch in dataloader: #tqdm():\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(batch)[0]\n",
        "\n",
        "        # If model output is not scalar, apply global spatial average pooling.\n",
        "        # This happens if you choose a dimensionality not equal 2048.\n",
        "        if pred.size(2) != 1 or pred.size(3) != 1:\n",
        "            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "\n",
        "        pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
        "\n",
        "        pred_arr[start_idx:start_idx + pred.shape[0]] = pred\n",
        "\n",
        "        start_idx = start_idx + pred.shape[0]\n",
        "\n",
        "    return pred_arr\n",
        "\n",
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \\\n",
        "        'Training and test mean vectors have different lengths'\n",
        "    assert sigma1.shape == sigma2.shape, \\\n",
        "        'Training and test covariances have different dimensions'\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    # Product might be almost singular\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    # Numerical error might give slight imaginary component\n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return (diff.dot(diff) + np.trace(sigma1)\n",
        "            + np.trace(sigma2) - 2 * tr_covmean)\n",
        "\n",
        "\n",
        "def calculate_activation_statistics(files, model, batch_size=50, dims=2048,\n",
        "                                    device='cpu'):\n",
        "    act = get_activations(files, model, batch_size, dims, device)\n",
        "    mu = np.mean(act, axis=0)\n",
        "    sigma = np.cov(act, rowvar=False)\n",
        "    return mu, sigma\n",
        "\n",
        "\n",
        "def compute_statistics_of_path(path, model, batch_size, dims, device):\n",
        "    if path.endswith('.npz'):\n",
        "        with np.load(path) as f:\n",
        "            m, s = f['mu'][:], f['sigma'][:]\n",
        "    else:\n",
        "        path = pathlib.Path(path)\n",
        "        files = sorted([file for ext in IMAGE_EXTENSIONS\n",
        "                       for file in path.glob('*.{}'.format(ext))])\n",
        "        m, s = calculate_activation_statistics(files, model, batch_size,\n",
        "                                               dims, device)\n",
        "\n",
        "    return m, s\n",
        "\n",
        "\n",
        "def calculate_fid_given_paths(paths, batch_size, device, dims):\n",
        "    \"\"\"Calculates the FID of two paths\"\"\"\n",
        "    for p in paths:\n",
        "        if not os.path.exists(p):\n",
        "            raise RuntimeError('Invalid path: %s' % p)\n",
        "\n",
        "    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
        "\n",
        "    model = InceptionV3([block_idx]).to(device)\n",
        "\n",
        "    m1, s1 = compute_statistics_of_path(paths[0], model, batch_size,\n",
        "                                        dims, device)\n",
        "    m2, s2 = compute_statistics_of_path(paths[1], model, batch_size,\n",
        "                                        dims, device)\n",
        "    fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n",
        "\n",
        "    return fid_value\n",
        "\n",
        "def get_fid(batch_size, dims, path):\n",
        "    device = torch.device('cuda' if (torch.cuda.is_available()) else 'cpu')\n",
        "    fid_value = calculate_fid_given_paths(path,\n",
        "                                          batch_size,\n",
        "                                          device,\n",
        "                                          dims)\n",
        "    return fid_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2WrDDhK9tT4"
      },
      "source": [
        "## Test code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvwKFKsf4um6"
      },
      "source": [
        "# test code, to read model and generate images \n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import torchvision.utils as vutils\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "from trainer import UNIT_Trainer\n",
        "from utils import get_config, pytorch03_to_pytorch04"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snmpk6WIRP3T"
      },
      "source": [
        "def generate_save_image(input, style, output_folder, file_name):\n",
        "  # currently passed in one image, need to loop through images\n",
        "    if torch.cuda.is_available():\n",
        "        image = Variable(transform(Image.open(input).convert('RGB')).unsqueeze(0).cuda())\n",
        "        style_image = Variable(\n",
        "            transform(Image.open(style).convert('RGB')).unsqueeze(0).cuda()) if style != '' else None\n",
        "    else:\n",
        "        image = Variable(transform(Image.open(input).convert('RGB')).unsqueeze(0))\n",
        "        style_image = Variable(\n",
        "            transform(Image.open(style).convert('RGB')).unsqueeze(0)) if style != '' else None\n",
        "\n",
        "    # Start testing - generate, need to change the image name later too\n",
        "    content, _ = encode(image)\n",
        "    outputs = decode(content)\n",
        "    outputs = (outputs + 1) / 2.\n",
        "    path = os.path.join(output_folder, file_name) #'output.jpg'\n",
        "    vutils.save_image(outputs.data, path, padding=0, normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtEMfq5Y6lDU"
      },
      "source": [
        "### Generate FID score not saving image to drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJdhxQOL6kVN"
      },
      "source": [
        "# arguments\n",
        "# --input inputs/gta_example.jpg --output_folder results/gta2city --checkpoint models/unit_gta2city.pt\n",
        "\n",
        "checkpoint_dir_summer2winter = '/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/'\n",
        "checkpoint_dir_monet2photo = '/content/drive/MyDrive/CSC2516_Project/UNIT_monet2photo_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/'\n",
        "\n",
        "# configs/unit_monet2photo_list.yaml\n",
        "config_file = 'configs/unit_summer2winter_yosemite256_list.yaml'\n",
        "output_folder = 'results/summer2winter' # need name addon\n",
        "csv_output_folder = '/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/results/'\n",
        "checkpoint = checkpoint_dir_summer2winter # or checkpoint_dir_monet2photo , need name add on \n",
        "output_only = True #only saving the generated image output\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--config', type=str, default=config_file, help=\"net configuration\")\n",
        "parser.add_argument('--style', type=str, default='', help=\"style image path\")\n",
        "# parser.add_argument('--a2b', type=int, default=1, help=\"1 for a2b and others for b2a\")\n",
        "parser.add_argument('--seed', type=int, default=10, help=\"random seed\")\n",
        "parser.add_argument('--num_style', type=int, default=10, help=\"number of styles to sample\")\n",
        "parser.add_argument('--synchronized', action='store_true', help=\"whether use synchronized style code or not\")\n",
        "parser.add_argument('--output_folder', type=str, default=output_folder, help=\"output image path\")\n",
        "# parser.add_argument('--output_only', action='store_true', help=\"whether use synchronized style code or not\")\n",
        "parser.add_argument('--output_path', type=str, default='.', help=\"path for logs, checkpoints, and VGG model weight\")\n",
        "parser.add_argument('--trainer', type=str, default='UNIT', help=\"UNIT\")\n",
        "parser.add_argument('-f', default='')\n",
        "opts = parser.parse_args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g7OBxppDT4h",
        "outputId": "ecd78200-3951-4eed-bd1f-58d322fd30a0"
      },
      "source": [
        "# manual set up hps, iterate through iteration numbers\n",
        "recon_kl_w = 0.1\n",
        "recon_kl_cyc_w = 0.01\n",
        "lr_value = 0.0001\n",
        "iterations = ['00008000', '00016000', '00024000', '00032000', '00040000', \n",
        "              '00048000', '00056000', '00064000', '00072000', '00080000']\n",
        "\n",
        "# load existing csv table or create new \n",
        "try:\n",
        "  FID_table = pd.read_csv(csv_output_folder + 'FID.csv')\n",
        "except: \n",
        "  FID_table =  pd.DataFrame(columns=['recon_kl_w', 'recon_kl_clc', 'lr_value', 'iteration', \n",
        "                                   'fid_val_A2B', 'fid_val_A2B', 'fid_val_B2A',\n",
        "                                   'fid_val_B2A2B'])\n",
        "torch.manual_seed(opts.seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(opts.seed)\n",
        "\n",
        "config = get_config(opts.config)\n",
        "\n",
        "# global definition \n",
        "input_folder = config['data_root']\n",
        "input_folder_A = input_folder + '/testA/'\n",
        "input_folder_B = input_folder + '/testB/'\n",
        "\n",
        "# iterate through different iterations\n",
        "for iter in iterations: \n",
        "  param_values = 'kl_w_' + str(recon_kl_w) + 'kl_clc_'+ str(recon_kl_cyc_w)  + '_lr_valie_' + str(lr_value)\n",
        "  current_output_A2B = opts.output_folder + '/A2B/' + param_values + '/'\n",
        "  current_output_A2B2A = opts.output_folder + '/A2B2A/' + param_values + '/'\n",
        "  current_output_B2A = opts.output_folder + '/B2A/' + param_values + '/'\n",
        "  current_output_B2A2B = opts.output_folder + '/B2A2B/' + param_values + '/'\n",
        "  local_string = 'gen_'+ str(iter) + '_batch_size_1_recon_kl_w_' \\\n",
        "                            + str(recon_kl_w) + '_recon_kl_clc_' \\\n",
        "                            + str(recon_kl_cyc_w)  + '_lr_value_' + str(lr_value) + '.pt'\n",
        "  current_checkpoint = os.path.join(checkpoint, local_string)\n",
        "  print(current_checkpoint)\n",
        "\n",
        "  # create output folder path \n",
        "  if not os.path.exists(opts.output_folder):\n",
        "      os.makedirs(opts.output_folder)\n",
        "  if not os.path.exists(current_output_A2B):\n",
        "      os.makedirs(current_output_A2B)\n",
        "  if not os.path.exists(current_output_A2B2A):\n",
        "      os.makedirs(current_output_A2B2A)\n",
        "  if not os.path.exists(current_output_B2A):\n",
        "      os.makedirs(current_output_B2A)\n",
        "  if not os.path.exists(current_output_B2A2B):\n",
        "      os.makedirs(current_output_B2A2B)\n",
        "\n",
        "  # Load experiment setting, modify config value \n",
        "  config = get_config(opts.config)\n",
        "  opts.num_style = 1 if opts.style != '' else opts.num_style\n",
        "\n",
        "  # Setup model and data loader\n",
        "  config['vgg_model_path'] = '.'\n",
        "\n",
        "  # loop through a2b and b2a \n",
        "  for a2b in [True, False]:\n",
        "    trainer = UNIT_Trainer(config)\n",
        "    try:\n",
        "        state_dict = torch.load(current_checkpoint)\n",
        "        trainer.gen_a.load_state_dict(state_dict['a'])\n",
        "        trainer.gen_b.load_state_dict(state_dict['b'])\n",
        "    except:\n",
        "        state_dict = pytorch03_to_pytorch04(torch.load(current_checkpoint))\n",
        "        trainer.gen_a.load_state_dict(state_dict['a'])\n",
        "        trainer.gen_b.load_state_dict(state_dict['b'])\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        trainer.cuda()\n",
        "    trainer.eval()\n",
        "    encode = trainer.gen_a.encode if a2b else trainer.gen_b.encode  # encode function\n",
        "    style_encode = trainer.gen_b.encode if a2b else trainer.gen_a.encode  # encode function\n",
        "    decode = trainer.gen_b.decode if a2b else trainer.gen_a.decode  # decode function\n",
        "\n",
        "    if 'new_size' in config:\n",
        "        new_size = config['new_size']\n",
        "    else:\n",
        "        if a2b:\n",
        "            new_size = config['new_size_a']\n",
        "        else:\n",
        "            new_size = config['new_size_b']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        transform = transforms.Compose([transforms.Resize(new_size),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        if a2b:\n",
        "          # 1. Generate images A -> B\n",
        "          image_list_A = os.listdir(input_folder_A)  # list of images inside A folder\n",
        "          for input_image in image_list_A:\n",
        "            generate_save_image(input_folder_A+input_image, opts.style, current_output_A2B, input_image)\n",
        "\n",
        "          # 2. Generate images A -> B -> A\n",
        "          image_list_A2B = os.listdir(current_output_A2B)  # list of images inside A2B folder\n",
        "          for input_image in image_list_A2B:\n",
        "            generate_save_image(current_output_A2B+input_image, opts.style, current_output_A2B2A, input_image)\n",
        "        else:\n",
        "          # 1. Generate images B -> A\n",
        "          image_list_B = os.listdir(input_folder_B)  # list of images inside B folder\n",
        "          for input_image in image_list_B:\n",
        "            generate_save_image(input_folder_B+input_image, opts.style, current_output_B2A, input_image)\n",
        "\n",
        "          # 2. Generate images B -> A -> B\n",
        "          image_list_B2A = os.listdir(current_output_B2A)  # list of images inside B2A folder\n",
        "          for input_image in image_list_B2A:\n",
        "            generate_save_image(current_output_B2A+input_image, opts.style, current_output_B2A2B, input_image)\n",
        "\n",
        "        \n",
        "        # compute fid score \n",
        "        if a2b:\n",
        "          # 1. compare A2B with testB\n",
        "          fid_val_A2B = get_fid(batch_size=1, dims=2048, path=[current_output_A2B, input_folder_B])\n",
        "\n",
        "          # 2. compare A2B2A with testA\n",
        "          fid_val_A2B2A = get_fid(batch_size=1, dims=2048, path=[current_output_A2B2A,input_folder_A])\n",
        "\n",
        "        else:\n",
        "          # 3. compare B2A with test A\n",
        "          fid_val_B2A = get_fid(batch_size=1, dims=2048, path=[current_output_B2A, input_folder_A])\n",
        "\n",
        "          # 4. compare B2A2B with test B\n",
        "          fid_val_B2A2B = get_fid(batch_size=1, dims=2048, path=[current_output_B2A2B, input_folder_B])\n",
        "\n",
        "  # append record after generating both A2B and B2A results \n",
        "  current_fid = {\n",
        "      'fid_val_A2B': fid_val_A2B,\n",
        "      'fid_val_A2B2A': fid_val_A2B2A,\n",
        "      'fid_val_B2A': fid_val_B2A,\n",
        "      'fid_val_B2A2B': fid_val_B2A2B,\n",
        "      'recon_kl_w': recon_kl_w,\n",
        "      'recon_kl_clc': recon_kl_cyc_w,\n",
        "      'lr_value': lr_value,\n",
        "      'iteration': iter\n",
        "      }\n",
        "\n",
        "  FID_table = FID_table.append(current_fid, ignore_index=True)\n",
        "\n",
        "  # save after each iteration\n",
        "  FID_table.to_csv(csv_output_folder + 'FID.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/gen_00008000_batch_size_1_recon_kl_w_0.1_recon_kl_clc_0.01_lr_value_0.0001.pt\n",
            "/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/gen_00016000_batch_size_1_recon_kl_w_0.1_recon_kl_clc_0.01_lr_value_0.0001.pt\n",
            "/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/gen_00024000_batch_size_1_recon_kl_w_0.1_recon_kl_clc_0.01_lr_value_0.0001.pt\n",
            "/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/gen_00032000_batch_size_1_recon_kl_w_0.1_recon_kl_clc_0.01_lr_value_0.0001.pt\n",
            "/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/gen_00040000_batch_size_1_recon_kl_w_0.1_recon_kl_clc_0.01_lr_value_0.0001.pt\n",
            "/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/gen_00048000_batch_size_1_recon_kl_w_0.1_recon_kl_clc_0.01_lr_value_0.0001.pt\n",
            "/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/gen_00056000_batch_size_1_recon_kl_w_0.1_recon_kl_clc_0.01_lr_value_0.0001.pt\n",
            "/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/gen_00064000_batch_size_1_recon_kl_w_0.1_recon_kl_clc_0.01_lr_value_0.0001.pt\n",
            "/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/gen_00072000_batch_size_1_recon_kl_w_0.1_recon_kl_clc_0.01_lr_value_0.0001.pt\n",
            "/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/gen_00080000_batch_size_1_recon_kl_w_0.1_recon_kl_clc_0.01_lr_value_0.0001.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rq_r7cG6qS1"
      },
      "source": [
        "### Using best hyperparameter to generate image to drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yAEwlE87Cpz"
      },
      "source": [
        "# arguments\n",
        "# --input inputs/gta_example.jpg --output_folder results/gta2city --checkpoint models/unit_gta2city.pt\n",
        "checkpoint_dir_summer2winter = '/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/'\n",
        "checkpoint_dir_monet2photo = '/content/drive/MyDrive/CSC2516_Project/UNIT_monet2photo_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/'\n",
        "\n",
        "# configs/unit_monet2photo_list.yaml\n",
        "config_file = 'configs/unit_summer2winter_yosemite256_list.yaml'\n",
        "output_folder = '/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/results/' # need name addon\n",
        "#csv_output_folder = '/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/results/'\n",
        "checkpoint = checkpoint_dir_summer2winter # or checkpoint_dir_monet2photo , need name add on \n",
        "output_only = True #only saving the generated image output\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--config', type=str, default=config_file, help=\"net configuration\")\n",
        "parser.add_argument('--style', type=str, default='', help=\"style image path\")\n",
        "# parser.add_argument('--a2b', type=int, default=1, help=\"1 for a2b and others for b2a\")\n",
        "parser.add_argument('--seed', type=int, default=10, help=\"random seed\")\n",
        "parser.add_argument('--num_style', type=int, default=10, help=\"number of styles to sample\")\n",
        "parser.add_argument('--synchronized', action='store_true', help=\"whether use synchronized style code or not\")\n",
        "parser.add_argument('--output_folder', type=str, default=output_folder, help=\"output image path\")\n",
        "# parser.add_argument('--output_only', action='store_true', help=\"whether use synchronized style code or not\")\n",
        "parser.add_argument('--output_path', type=str, default='.', help=\"path for logs, checkpoints, and VGG model weight\")\n",
        "parser.add_argument('--trainer', type=str, default='UNIT', help=\"UNIT\")\n",
        "parser.add_argument('-f', default='')\n",
        "opts = parser.parse_args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vifEQQHh7Z_j",
        "outputId": "5d99acee-b594-48a1-ce96-9abfc850acd4"
      },
      "source": [
        "# manual set up hps, iterate through iteration numbers\n",
        "recon_kl_w = 0.01\n",
        "recon_kl_cyc_w = 0.1\n",
        "lr_value = 0.0001\n",
        "iter = '00024000'\n",
        "\n",
        "torch.manual_seed(opts.seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(opts.seed)\n",
        "\n",
        "config = get_config(opts.config)\n",
        "\n",
        "# global definition \n",
        "input_folder = config['data_root']\n",
        "input_folder_A = input_folder + '/testA/'\n",
        "input_folder_B = input_folder + '/testB/'\n",
        "\n",
        "param_values = 'kl_w_' + str(recon_kl_w) + 'kl_clc_'+ str(recon_kl_cyc_w)  + '_lr_valie_' + str(lr_value)\n",
        "current_output_A2B = opts.output_folder + '/A2B/' + param_values + '/'\n",
        "current_output_A2B2A = opts.output_folder + '/A2B2A/' + param_values + '/'\n",
        "current_output_B2A = opts.output_folder + '/B2A/' + param_values + '/'\n",
        "current_output_B2A2B = opts.output_folder + '/B2A2B/' + param_values + '/'\n",
        "local_string = 'gen_'+ str(iter) + '_batch_size_1_recon_kl_w_' \\\n",
        "                          + str(recon_kl_w) + '_recon_kl_clc_' \\\n",
        "                          + str(recon_kl_cyc_w)  + '_lr_value_' + str(lr_value) + '.pt'\n",
        "current_checkpoint = os.path.join(checkpoint, local_string)\n",
        "print(current_checkpoint)\n",
        "\n",
        "# create output folder path \n",
        "if not os.path.exists(opts.output_folder):\n",
        "    os.makedirs(opts.output_folder)\n",
        "if not os.path.exists(current_output_A2B):\n",
        "    os.makedirs(current_output_A2B)\n",
        "if not os.path.exists(current_output_A2B2A):\n",
        "    os.makedirs(current_output_A2B2A)\n",
        "if not os.path.exists(current_output_B2A):\n",
        "    os.makedirs(current_output_B2A)\n",
        "if not os.path.exists(current_output_B2A2B):\n",
        "    os.makedirs(current_output_B2A2B)\n",
        "\n",
        "# Load experiment setting, modify config value \n",
        "config = get_config(opts.config)\n",
        "opts.num_style = 1 if opts.style != '' else opts.num_style\n",
        "\n",
        "# Setup model and data loader\n",
        "config['vgg_model_path'] = '.'\n",
        "\n",
        "# loop through a2b and b2a \n",
        "for a2b in [True, False]:\n",
        "  trainer = UNIT_Trainer(config)\n",
        "  try:\n",
        "      state_dict = torch.load(current_checkpoint)\n",
        "      trainer.gen_a.load_state_dict(state_dict['a'])\n",
        "      trainer.gen_b.load_state_dict(state_dict['b'])\n",
        "  except:\n",
        "      state_dict = pytorch03_to_pytorch04(torch.load(current_checkpoint))\n",
        "      trainer.gen_a.load_state_dict(state_dict['a'])\n",
        "      trainer.gen_b.load_state_dict(state_dict['b'])\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "      trainer.cuda()\n",
        "  trainer.eval()\n",
        "  encode = trainer.gen_a.encode if a2b else trainer.gen_b.encode  # encode function\n",
        "  style_encode = trainer.gen_b.encode if a2b else trainer.gen_a.encode  # encode function\n",
        "  decode = trainer.gen_b.decode if a2b else trainer.gen_a.decode  # decode function\n",
        "\n",
        "  if 'new_size' in config:\n",
        "      new_size = config['new_size']\n",
        "  else:\n",
        "      if a2b:\n",
        "          new_size = config['new_size_a']\n",
        "      else:\n",
        "          new_size = config['new_size_b']\n",
        "\n",
        "  with torch.no_grad():\n",
        "      transform = transforms.Compose([transforms.Resize(new_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "      if a2b:\n",
        "        # 1. Generate images A -> B\n",
        "        image_list_A = os.listdir(input_folder_A)  # list of images inside A folder\n",
        "        for input_image in image_list_A:\n",
        "          generate_save_image(input_folder_A+input_image, opts.style, current_output_A2B, input_image)\n",
        "\n",
        "        # 2. Generate images A -> B -> A\n",
        "        image_list_A2B = os.listdir(current_output_A2B)  # list of images inside A2B folder\n",
        "        for input_image in image_list_A2B:\n",
        "          generate_save_image(current_output_A2B+input_image, opts.style, current_output_A2B2A, input_image)\n",
        "      else:\n",
        "        # 1. Generate images B -> A\n",
        "        image_list_B = os.listdir(input_folder_B)  # list of images inside B folder\n",
        "        for input_image in image_list_B:\n",
        "          generate_save_image(input_folder_B+input_image, opts.style, current_output_B2A, input_image)\n",
        "\n",
        "        # 2. Generate images B -> A -> B\n",
        "        image_list_B2A = os.listdir(current_output_B2A)  # list of images inside B2A folder\n",
        "        for input_image in image_list_B2A:\n",
        "          generate_save_image(current_output_B2A+input_image, opts.style, current_output_B2A2B, input_image)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CSC2516_Project/UNIT_summer2winter_small/outputs/unit_summer2winter_yosemite256_folder/checkpoints/gen_00024000_batch_size_1_recon_kl_w_0.01_recon_kl_clc_0.1_lr_value_0.0001.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbXVE6oahP4U"
      },
      "source": [
        "FID_table = pd.read_csv(output_folder + 'FID.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQRYsCDbxcXx"
      },
      "source": [
        "FID_table.reset_index().drop(columns=['index']).to_csv(csv_output_folder + 'FID.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "8ygNb6HQyPxk",
        "outputId": "bc9ddc35-d083-449d-c53e-247e8595a8bb"
      },
      "source": [
        "FID_table.reset_index().drop(columns=['index'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>recon_kl_w</th>\n",
              "      <th>recon_kl_clc</th>\n",
              "      <th>lr_value</th>\n",
              "      <th>iteration</th>\n",
              "      <th>fid_val_A2B</th>\n",
              "      <th>fid_val_B2A</th>\n",
              "      <th>fid_val_B2A2B</th>\n",
              "      <th>fid_val_A2B2A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>8000</td>\n",
              "      <td>136.012255</td>\n",
              "      <td>134.220830</td>\n",
              "      <td>122.235057</td>\n",
              "      <td>135.282256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>16000</td>\n",
              "      <td>146.076901</td>\n",
              "      <td>115.355357</td>\n",
              "      <td>113.190032</td>\n",
              "      <td>146.293416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>24000</td>\n",
              "      <td>136.953084</td>\n",
              "      <td>122.913594</td>\n",
              "      <td>135.937257</td>\n",
              "      <td>144.920483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>32000</td>\n",
              "      <td>134.728458</td>\n",
              "      <td>130.966335</td>\n",
              "      <td>154.857080</td>\n",
              "      <td>140.022672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>40000</td>\n",
              "      <td>132.782427</td>\n",
              "      <td>117.296910</td>\n",
              "      <td>157.169483</td>\n",
              "      <td>147.483740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>48000</td>\n",
              "      <td>138.792552</td>\n",
              "      <td>117.460680</td>\n",
              "      <td>143.309153</td>\n",
              "      <td>156.810699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>56000</td>\n",
              "      <td>131.431238</td>\n",
              "      <td>117.931405</td>\n",
              "      <td>141.219361</td>\n",
              "      <td>145.712265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>64000</td>\n",
              "      <td>131.149544</td>\n",
              "      <td>121.012343</td>\n",
              "      <td>150.742270</td>\n",
              "      <td>144.106042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>72000</td>\n",
              "      <td>143.876309</td>\n",
              "      <td>126.596087</td>\n",
              "      <td>151.668231</td>\n",
              "      <td>161.442260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>80000</td>\n",
              "      <td>152.830741</td>\n",
              "      <td>119.574793</td>\n",
              "      <td>149.809497</td>\n",
              "      <td>175.090845</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   recon_kl_w  recon_kl_clc  ...  fid_val_B2A2B  fid_val_A2B2A\n",
              "0        0.01          0.01  ...     122.235057     135.282256\n",
              "1        0.01          0.01  ...     113.190032     146.293416\n",
              "2        0.01          0.01  ...     135.937257     144.920483\n",
              "3        0.01          0.01  ...     154.857080     140.022672\n",
              "4        0.01          0.01  ...     157.169483     147.483740\n",
              "5        0.01          0.01  ...     143.309153     156.810699\n",
              "6        0.01          0.01  ...     141.219361     145.712265\n",
              "7        0.01          0.01  ...     150.742270     144.106042\n",
              "8        0.01          0.01  ...     151.668231     161.442260\n",
              "9        0.01          0.01  ...     149.809497     175.090845\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqr4xzkgxS0V"
      },
      "source": [
        "FID_table = FID_table.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-otqnNljxKXl"
      },
      "source": [
        "FID_table = FID_table.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV695euRwG6_"
      },
      "source": [
        "FID_table = FID_table.drop(columns=['fid_val_A2B.1'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}